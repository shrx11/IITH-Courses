{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment-4-Codes.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wg0rBLqt0GLU"
      },
      "source": [
        "CS19BTECH11020"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhcvcJVN0JYb"
      },
      "source": [
        "Q5 (a) Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqEzd_9V0RFB"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "from numpy import log, dot, e"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVHD8nhKEj0x"
      },
      "source": [
        "def sigmoid(z): \n",
        "      return 1 / (1 + np.exp(-z))"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMUo2yYw0Ylq"
      },
      "source": [
        "class LogisticRegression:\n",
        "    \n",
        "    def cost_function(self):                 \n",
        "        z = self.X.dot(self.weights)+self.b        \n",
        "        pred = sigmoid(z)   \n",
        "        temp= pred-self.Y.T\n",
        "        tmp = np.reshape( temp, self.m )  \n",
        "\n",
        "        self.cost = np.sum(self.Y * log(sigmoid(z))+(1 - self.Y) * log(1 - sigmoid(z)))/self.m\n",
        "        \n",
        "\n",
        "        self.dW = np.dot( self.X.T, tmp ) / self.m         \n",
        "        self.db = np.sum( tmp ) / self.m\n",
        "        return self\n",
        "    \n",
        "    def fit(self, X, Y, epochs=25, lr=0.05,weights=np.zeros(2),b=0):        \n",
        "        cost_list = []\n",
        "        self.m, self.n = X.shape        \n",
        "        # weight initialization        \n",
        "        self.weights = weights        \n",
        "        self.b = b        \n",
        "        self.X = X        \n",
        "        self.Y = Y\n",
        "        self.learning_rate = lr      \n",
        "        self.epochs = epochs\n",
        "\n",
        "        # gradient descent learning        \n",
        "        for i in range( self.epochs ) :     \n",
        "            self.cost_function()    \n",
        "            cost_list.append(self.cost)\n",
        "            self.weights = self.weights - self.learning_rate * self.dW    \n",
        "            self.b = self.b - self.learning_rate * self.db \n",
        "\n",
        "        self.cost_list=cost_list           \n",
        "        return self\n",
        "    \n",
        "    def predict(self, X):        \n",
        "        z = X.dot(self.weights)+self.b\n",
        "        Z = 1/(1+np.exp(-(z)))\n",
        "        Y = np.where( Z > 0.5, 1, 0 )        \n",
        "        return Y"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8gaOLqhkY1D"
      },
      "source": [
        "5 (b) fθ(x1, x2) = θ0 + θ1x1 + θ2x2,    initializing weights as θ0 = −1, θ1 = 1.5, θ2 = 0.5 and learning rate as 0.1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nx40RJb9lwl0"
      },
      "source": [
        "Initialising train and test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LunSQfJLBb3E"
      },
      "source": [
        "X_train=np.array([['0.346', '0.78'],\n",
        "       ['0.303', '0.439'],\n",
        "       ['0.358', '0.729'],\n",
        "       ['0.602', '0.863'],\n",
        "       ['0.79', '0.753'],\n",
        "       ['0.611', '0.965']]).astype(float)\n",
        "\n",
        "X_test=np.array([['0.959', '0.382'],\n",
        "       ['0.75', '0.306'],\n",
        "       ['0.395', '0.76'],\n",
        "       ['0.823', '0.764'],\n",
        "       ['0.761', '0.874'],\n",
        "       ['0.844', '0.435']]).astype(float)\n",
        "\n",
        "y_train=np.array([0,0,0,1,1,1]).astype(float)\n",
        "y_test=np.array([0,0,0,1,1,1]).astype(float)"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8w1HZbZTl56T"
      },
      "source": [
        "**(i)** logistic model P(ˆy = 1|x1, x2) and its cross-entropy error function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9gbWCyC37Gx",
        "outputId": "b048a446-32b8-4685-f610-86174c479a35"
      },
      "source": [
        "logreg = LogisticRegression()\n",
        "logreg.fit(X_train, y_train, epochs=5001, lr=0.05)\n",
        "y_pred = logreg.predict(X_test)\n",
        "print(metrics.accuracy_score(y_test, y_pred))\n",
        "print(\"Logistic regression model  W=\", logreg.weights,\", b=\",logreg.b)\n",
        "\n",
        "for i in range(len(logreg.cost_list)):\n",
        "  if(i%200 == 0):\n",
        "    print(\"Epoch: \" ,i,\" Loss= \",logreg.cost_list[i] )\n",
        "  "
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6666666666666666\n",
            "Logistic regression model  W= [8.2860965  2.42928631] , b= -5.895232687237986\n",
            "Epoch:  0  Loss=  -0.6931471805599453\n",
            "Epoch:  200  Loss=  -0.6336049880786752\n",
            "Epoch:  400  Loss=  -0.5849779704868059\n",
            "Epoch:  600  Loss=  -0.5422712682948627\n",
            "Epoch:  800  Loss=  -0.5046626269625096\n",
            "Epoch:  1000  Loss=  -0.47143172960981133\n",
            "Epoch:  1200  Loss=  -0.44195701563626977\n",
            "Epoch:  1400  Loss=  -0.41570763419647266\n",
            "Epoch:  1600  Loss=  -0.39223307692049025\n",
            "Epoch:  1800  Loss=  -0.3711523176194232\n",
            "Epoch:  2000  Loss=  -0.35214348834466674\n",
            "Epoch:  2200  Loss=  -0.3349345895567221\n",
            "Epoch:  2400  Loss=  -0.3192954078490455\n",
            "Epoch:  2600  Loss=  -0.30503063336427944\n",
            "Epoch:  2800  Loss=  -0.2919740785866871\n",
            "Epoch:  3000  Loss=  -0.27998386367222033\n",
            "Epoch:  3200  Loss=  -0.26893842682998453\n",
            "Epoch:  3400  Loss=  -0.2587332267479291\n",
            "Epoch:  3600  Loss=  -0.24927801908552047\n",
            "Epoch:  3800  Loss=  -0.24049460592120406\n",
            "Epoch:  4000  Loss=  -0.23231497333347165\n",
            "Epoch:  4200  Loss=  -0.2246797469239634\n",
            "Epoch:  4400  Loss=  -0.2175369076951931\n",
            "Epoch:  4600  Loss=  -0.2108407212821074\n",
            "Epoch:  4800  Loss=  -0.20455084228630893\n",
            "Epoch:  5000  Loss=  -0.1986315626188717\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlxLoIb2ptT7"
      },
      "source": [
        "**(ii)** Use gradient descent to update θ0, θ1, θ2 for one iteration. Write down\n",
        "the updated logistic regression model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7C7OIB6mamV",
        "outputId": "83d226e4-1c52-437a-c429-41e6eb56364a"
      },
      "source": [
        "logreg = LogisticRegression()\n",
        "W=np.array([1.5,0.5])\n",
        "logreg.fit(X_train, y_train, epochs=1, lr=0.1,weights=W,b=-1)\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "print(\"Updated logistic regression model  W=\", logreg.weights,\", b=\",logreg.b)"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated logistic regression model  W= [1.50535086 0.50196867] , b= -1.0031662597725644\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ixOSS_QmHY1"
      },
      "source": [
        "**(iii)**At convergence of gradient descent, use the model to make predictions\n",
        "for all the samples in the test dataset. Calculate and report the accuracy, precision\n",
        "and recall to evaluate this model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzVuuFBIGg4A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "822abee3-b760-4d7d-f778-3aebc7332e73"
      },
      "source": [
        "logreg = LogisticRegression()\n",
        "logreg.fit(X_train, y_train, epochs=1000, lr=0.05)\n",
        "y_pred = logreg.predict(X_test)\n",
        "print(\"Accuracy: \",metrics.accuracy_score(y_test, y_pred))\n",
        "print(\"Precision: \",metrics.precision_score(y_test, y_pred))\n",
        "print(\"Recall: \",metrics.recall_score(y_test, y_pred))\n"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.6666666666666666\n",
            "Precision:  0.6\n",
            "Recall:  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68SGJc4NuTId"
      },
      "source": [
        "#Q 6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vqs6jOjWuRqn"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score, mean_squared_error"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHkuINKTuRwR"
      },
      "source": [
        "filename=\"/content/drive/MyDrive/new-york-city-taxi-fare-prediction/train.csv\"\n",
        "n=1000000\n",
        "train_df = pd.read_csv(filename, nrows=n)\n",
        "n1=100000\n",
        "#valid_df = pd.read_csv(filename, skiprows=n,nrows=n1,names=[\"key\" ,'fare_amount'\t,'pickup_datetime',\t'pickup_longitude',\t'pickup_latitude','dropoff_longitude','dropoff_latitude','passenger_count'])\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/new-york-city-taxi-fare-prediction/test.csv')"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MkPhqhJ-iu6"
      },
      "source": [
        "train_df.dropna(inplace=True)\n",
        "#valid_df.dropna(inplace=True)\n",
        "test_df.dropna(inplace=True)"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19djY73LZEwz"
      },
      "source": [
        "from math import radians, cos, sin, asin, sqrt\n",
        "def haversine_np(lon1, lat1, lon2, lat2):\n",
        "  \n",
        "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
        "    dlon = lon2 - lon1\n",
        "    dlat = lat2 - lat1\n",
        "\n",
        "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
        "    c = 2 * np.arcsin(np.sqrt(a))\n",
        "    km = 6367 * c\n",
        "    return km\n",
        "\n",
        "def add_trip_distance(df):\n",
        "    df['trip_distance'] = haversine_np(df['pickup_longitude'], df['pickup_latitude'], df['dropoff_longitude'], df['dropoff_latitude'])"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IURvNGXTZpnP"
      },
      "source": [
        "def clean_df(df):\n",
        "    #reverse incorrectly assigned longitude/latitude values\n",
        "    df = df.assign(rev=df.dropoff_latitude<df.dropoff_longitude)\n",
        "    idx = (df['rev'] == 1)\n",
        "    df.loc[idx,['dropoff_longitude','dropoff_latitude']] = df.loc[idx,['dropoff_latitude','dropoff_longitude']].values\n",
        "    df.loc[idx,['pickup_longitude','pickup_latitude']] = df.loc[idx,['pickup_latitude','pickup_longitude']].values\n",
        "    \n",
        "    #remove outliers\n",
        "    criteria = (\n",
        "    \" 0 < fare_amount <= 500\"\n",
        "    \" and 0 < passenger_count <= 6 \"\n",
        "    \" and -75 <= pickup_longitude <= -72 \"\n",
        "    \" and -75 <= dropoff_longitude <= -72 \"\n",
        "    \" and 40 <= pickup_latitude <= 42 \"\n",
        "    \" and 40 <= dropoff_latitude <= 42 \"\n",
        "    )\n",
        "    df = (df\n",
        "          .dropna()\n",
        "          .query(criteria)\n",
        "          .reset_index()\n",
        "          .drop(columns=['rev', 'index'])          \n",
        "         )\n",
        "    return df"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qWPVb7rZYP7"
      },
      "source": [
        "def datetime_info(df):\n",
        "    #Convert to datetime format\n",
        "    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'],format=\"%Y-%m-%d %H:%M:%S UTC\")\n",
        "    \n",
        "    df['hour'] = df.pickup_datetime.dt.hour\n",
        "    df['day'] = df.pickup_datetime.dt.day\n",
        "    df['month'] = df.pickup_datetime.dt.month\n",
        "    df['weekday'] = df.pickup_datetime.dt.weekday\n",
        "    df['year'] = df.pickup_datetime.dt.year\n",
        "    \n",
        "    return df"
      ],
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvFSYo5DZpp1"
      },
      "source": [
        "clean_df(train_df)\n",
        "add_trip_distance(train_df)\n",
        "add_trip_distance(test_df)\n",
        "train_df=datetime_info(train_df)\n",
        "test_df=datetime_info(test_df)\n",
        "train_df.drop(['pickup_datetime'],axis=1,inplace=True)\n",
        "test_df.drop(['pickup_datetime'],axis=1,inplace=True)\n",
        "\n",
        "#clean_df(valid_df)\n",
        "#add_trip_distance(valid_df)\n",
        "#valid_df=datetime_info(valid_df)\n",
        "#valid_df.drop(['pickup_datetime'],axis=1,inplace=True)"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rq7-OGJuuR5C"
      },
      "source": [
        "X_train = train_df.drop(['fare_amount','key'], axis=1)\n",
        "X_test = test_df.drop(['key'], axis=1)\n",
        "y_train=train_df.fare_amount\n",
        "\n",
        "#X_valid = valid_df.drop(['fare_amount','key'], axis=1)\n",
        "#y_valid=vaid_df.fare_amount"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvVE-KggEG5o"
      },
      "source": [
        "import xgboost as xgb\n",
        "reg = xgb.XGBRegressor(objective='reg:squarederror',n_estimators=100, max_depth=5, n_jobs=-1, random_state=42)\n",
        "reg.fit(X_train, y_train)\n",
        "y_test_pred = reg.predict(X_test)\n",
        "# to run rmse on valid set\n",
        "#y_valid_pred = reg.predict(X_valid)\n",
        "#rmse = mean_squared_error(y_valid, y_valid_pred) ** 0.5\n",
        "#print('RMSE:', rmse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1ooE-h6EXQD"
      },
      "source": [
        "data= {\n",
        "    'key':test_df.key,\n",
        "    'fare_amount': y_test_pred\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "df.to_csv('/content/drive/MyDrive/new-york-city-taxi-fare-prediction/submission(1).csv', sep=',', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0G3tUQmEXSo"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "regr = RandomForestRegressor(n_estimators=100,max_depth=5, max_features=7,random_state=0)\n",
        "regr.fit(X_train, y_train)\n",
        "y_test_pred = regr.predict(X_test)\n",
        "# to run rmse on valid set\n",
        "#y_valid_pred = regr.predict(X_valid)\n",
        "#rmse = mean_squared_error(y_valid, y_valid_pred) ** 0.5\n",
        "#print('RMSE:', rmse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sePtMVrw03KU"
      },
      "source": [
        "data= {\n",
        "    'key':test_df.key,\n",
        "    'fare_amount': y_test_pred\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "df.to_csv('/content/drive/MyDrive/new-york-city-taxi-fare-prediction/submission(2).csv', sep=',', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}